

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Image Use Case &#8212; Data Science for Copernicus Data</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AI Applications in Remote Sensing" href="other_usecases.html" />
    <link rel="prev" title="Crop Use Case" href="crop_dataset.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Data Science for Copernicus Data</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Data Science for Copernicus Data
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="program.html">
   Preliminary Program
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="crop_dataset.html">
   Crop Use Case
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Image Use Case
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_usecases.html">
   AI Applications in Remote Sensing
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/image_dataset.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset-preparation">
   Dataset Preparation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-inspection-visualization-and-preprocessing">
   Data Inspection, Visualization and Preprocessing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-inspection">
     Data Inspection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-preprocessing">
     Data Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-visualization">
     Data Visualization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-object-based-segmentation">
   Unsupervised Object-Based Segmentation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-pixel-wise-classification">
   Supervised Pixel-Wise Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-hyperparameter-estimation-grid-search-vs-random-search">
   Model Hyperparameter Estimation (Grid Search vs. Random Search)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-dl-based-regression">
   Supervised DL-Based Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references-use-case-2">
   References Use Case 2
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="image-use-case">
<h1>Image Use Case<a class="headerlink" href="#image-use-case" title="Permalink to this headline">¶</a></h1>
<p>The objective of this use case is to familiraize the participants with the unstructrued datasets (image data).</p>
<div class="section" id="dataset-preparation">
<h2>Dataset Preparation<a class="headerlink" href="#dataset-preparation" title="Permalink to this headline">¶</a></h2>
<p>To prepare the dataset, follow these steps:</p>
<ol class="simple">
<li><p>Download Sentinel-2 images from Google Cloud Storage using <span class="xref myst">the Google Cloud Storage download notebook</span>.</p></li>
<li><p>Preprocess the downloaded images using <span class="xref myst">the Sentinel-2 preprocessing notebook</span>.</p></li>
<li><p>Rasterize the shapefile containing the training labels using <span class="xref myst">the rasterization notebook</span>.</p></li>
</ol>
</div>
<div class="section" id="data-inspection-visualization-and-preprocessing">
<h2>Data Inspection, Visualization and Preprocessing<a class="headerlink" href="#data-inspection-visualization-and-preprocessing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-inspection">
<h3>Data Inspection<a class="headerlink" href="#data-inspection" title="Permalink to this headline">¶</a></h3>
<p>This step is mandatory in all machine learning experiments. It allows discovering the dataset characterstics, its errors such as missing data in order to be correctly processed. It also allows exploring other properties of the data such as the distribution.</p>
</div>
<div class="section" id="data-preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">¶</a></h3>
<p><strong>Normalization</strong></p>
<p>Normalization is a preprocessing steps that must be applied (in most cases) to datasets before using any machine learning algorithm. The objective of the different normalization stratigies is to bring all features in a dataset to common scales.</p>
<ul class="simple">
<li><p>Standarization</p></li>
</ul>
<p><img alt="alt text" src="_images/standarization.png" /></p>
<ul class="simple">
<li><p>Min Max scaling</p></li>
</ul>
<p><img alt="alt text" src="_images/min_max.png" /></p>
<p>In this use case, we will use the Min Max scaling technique in order to normalize our datatset.</p>
<p><strong>Missing data</strong></p>
<ul class="simple">
<li><p>Inpainting: In art, this process aims at conserving artworks that are damaged or deteriorating. It is performed by a trained art conservator who carefully studied the artwork to determine the missing pieces. In this exercise, we will be using OpenCV<sup><a class="reference external" href="#opencv">*</a></sup> to perform the image inpainting. In this library, two inpainting techniques are implemented, cv.INPAINT_TELEA<sup><a class="reference external" href="#inpaint1">*</a></sup> and cv.INPAINT_NS<sup><a class="reference external" href="#inpaint2">*</a></sup> inspired from fluid dynamics.</p></li>
</ul>
<p><img alt="alt text" src="_images/inpaint_example.png" /></p>
<p>*Figure from <sup><a class="reference external" href="#inpaint2">*</a></sup> *</p>
</div>
<div class="section" id="data-visualization">
<h3>Data Visualization<a class="headerlink" href="#data-visualization" title="Permalink to this headline">¶</a></h3>
<p>In this use case, we will concentrate on the use of interactive matplotlib<sup><a class="reference external" href="#matplotlib">*</a></sup> to visualize the images we will work on.</p>
</div>
</div>
<div class="section" id="unsupervised-object-based-segmentation">
<h2>Unsupervised Object-Based Segmentation<a class="headerlink" href="#unsupervised-object-based-segmentation" title="Permalink to this headline">¶</a></h2>
<p>Unsuperivsed object-based segmentation (also known as super-pixel segmentation) techniques use spectral (as in classical pixel-based techniques) and spatial information such as shape, texture, spatial connexity in an image to group similar pixels. Multiple algorithms have been developed for this problem such as Felzenszwalb’s efficient graph<sup><a class="reference external" href="#inpaint2">*</a></sup> *, Quickshift<sup><a class="reference external" href="#inpaint2">*</a></sup> * and SLIC<sup><a class="reference external" href="#inpaint2">*</a></sup> * algorithms.</p>
<p><img alt="alt text" src="_images/super_pixel.png" /></p>
<p>*Figure from <sup><a class="reference external" href="#super_pixel">*</a></sup> *</p>
<p>In this use case, we will use the Felzenszwalb algorithm. It is a graph-based algorithm that sees every pixel in an image as a vertex in the graph and edges connect these vertices. Hence, the semgnetation reduces to finding communites in the vertices. Boundaries are used to separate communities of pixels. A bounday is where similarity ends and dissimilarity begins. The method uses a predicate function to measure the dissimiarlity between two components. At the first step of the algorithm, each pixel (vertex) represents a component. Then, the predicate function is used to aggregate similar components where edges between vertices in the same component have low weights and higher weight between vertices in different components.</p>
</div>
<div class="section" id="supervised-pixel-wise-classification">
<h2>Supervised Pixel-Wise Classification<a class="headerlink" href="#supervised-pixel-wise-classification" title="Permalink to this headline">¶</a></h2>
<p><img alt="alt text" src="_images/supervised_1.png" /></p>
<p><img alt="alt text" src="_images/supervised_2.png" /></p>
<p><img alt="alt text" src="_images/supervised_3.png" /></p>
<p><img alt="alt text" src="_images/supervised_4.png" /></p>
<p><img alt="alt text" src="_images/supervised_5.png" /></p>
<p><img alt="alt text" src="_images/supervised_6.png" /></p>
</div>
<div class="section" id="model-hyperparameter-estimation-grid-search-vs-random-search">
<h2>Model Hyperparameter Estimation (Grid Search vs. Random Search)<a class="headerlink" href="#model-hyperparameter-estimation-grid-search-vs-random-search" title="Permalink to this headline">¶</a></h2>
<p>Grid search<sup><a class="reference external" href="#grid_search">*</a></sup> is an exahstive method to set the hyperparameters of a machine learning model. In this technique, a set of ranges for the hyperparameters are selected by the practicioner. The method then examines every possible combination before selecting the best one. On the other hand, random search<sup><a class="reference external" href="#random_search">*</a></sup> selects random combinations from the input ranges to be tested and validated.</p>
<p><img alt="alt text" src="_images/param_modele.jpeg" /></p>
</div>
<div class="section" id="supervised-dl-based-regression">
<h2>Supervised DL-Based Regression<a class="headerlink" href="#supervised-dl-based-regression" title="Permalink to this headline">¶</a></h2>
<p>The LAI is a biophysical parameter that measures the total area of leaves per unit ground area and is directly correlated with the amount of intercepted light by the plant. This parameter has many uses such as the prediction of  photosynthetic primary production, monitoring crop growth and yield estimation. Moreover, the LAI is required by  many global models of  climate, ecosystem productivity and  ecology.</p>
<p>The existing methods are too complex and take a long time for inference (physical inversion models). No operational models exist to estimate the LAI of Sentinel-2 products as in the MODIS LAI products.</p>
<p>With DL we can obtain LAI values similar to those obtained by SNAP but in around 17 seconds compared to 15 mins by SNAP.</p>
<p><img alt="alt text" src="_images/training_testing.png" /></p>
<p><em>Training and testing tiles</em></p>
<p><img alt="alt text" src="_images/training_dates.png" /></p>
<p><em>Training dates</em></p>
<p><img alt="alt text" src="_images/testing_dates.png" /></p>
<p><em>Testing dates</em></p>
<p><img alt="alt text" src="_images/unet.png" /></p>
<p><em>UNet architecture</em></p>
<p><img alt="alt text" src="_images/model_training.png" /></p>
<p><em>Training procedure</em></p>
<p><img alt="alt text" src="_images/model_inference.png" /></p>
<p><em>Inference procedure</em></p>
<p><img alt="alt text" src="_images/T30TYM.png" /></p>
<p><em>Estimation of LAI values for Sentinel-2 images of the T30TYM tile</em></p>
<p><img alt="alt text" src="_images/snap_unet.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance</em></p>
<p><img alt="alt text" src="_images/snap_unet2.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance</em></p>
<p><img alt="alt text" src="_images/spring_crops.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance on spring crops</em></p>
<p><img alt="alt text" src="_images/winter_crops.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance on winter crops</em></p>
<p><img alt="alt text" src="_images/summer_crops.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance on summer crops</em></p>
<p><img alt="alt text" src="_images/permanent_crops.png" /></p>
<p><em>Visualization of SNAP vs. UNet performance on permanent crops</em></p>
<p><em>Comparison with SNAP</em></p>
</div>
<div class="section" id="references-use-case-2">
<h2>References Use Case 2<a class="headerlink" href="#references-use-case-2" title="Permalink to this headline">¶</a></h2>
<p><a name="opencv">*</a> https://docs.opencv.org/master/df/d3d/tutorial_py_inpainting.html</p>
<p><a name="inpaint1">*</a> Telea, Alexandru. “An image inpainting technique based on the fast marching method.” Journal of graphics tools 9.1 (2004): 23-34.</p>
<p><a name="inpaint2">*</a> Bertalmio, Marcelo, Andrea L. Bertozzi, and Guillermo Sapiro. “Navier-stokes, fluid dynamics, and image and video inpainting.” In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, vol. 1, pp. I-355. IEEE, 2001.</p>
<p><a name="felzen">*</a> Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004.</p>
<p><a name="quick_shift">*</a> Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008.</p>
<p><a name="slic">*</a> Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012.</p>
<p><a name="super_pixel">*</a> https://scikit-image.org/docs/0.12.x/auto_examples/segmentation/plot_segmentations.html</p>
<p><a name="random_search">*</a> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html</p>
<p><a name="grid_search">*</a> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="crop_dataset.html" title="previous page">Crop Use Case</a>
    <a class='right-next' id="next-link" href="other_usecases.html" title="next page">AI Applications in Remote Sensing</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mohanad Albughdadi<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>